{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"12xbBeBldKFlRaE1P3fMzvcf5vbmTmwo5","timestamp":1763392279336}],"toc_visible":true,"authorship_tag":"ABX9TyO19YOaV0R+tILVBjNDbBIV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hierarchical Multi-Scale Latent VAE"],"metadata":{"id":"mBjEZ1z-jhGd"}},{"cell_type":"markdown","source":["- This is the most theoretically interesting for emergent representation learning\n"," -Stochastic bottleneck forces the model to develop a compressed “language” rather than just memorising patterns.\n","\n","- Complete Colab Notebook: Hierarchical VAE for Emergent Representation Learning"],"metadata":{"id":"c-4-B0UIiguZ"}},{"cell_type":"markdown","source":["## Part 1: ARHITECRE, TRAINING, & β-ANNEALING."],"metadata":{"id":"nH7wv_cQhwMk"}},{"cell_type":"markdown","source":["Cell 1: Environment Setup"],"metadata":{"id":"9_t83mdefme-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnmqgKZoffFy"},"outputs":[],"source":["# Check hardware\n","import torch\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","# Install dependencies\n","!pip install biopython umap-learn scikit-learn matplotlib seaborn tqdm -q\n","\n","print(\"✓ Environment ready\")"]},{"cell_type":"markdown","source":["Cell 2: Import Libraries"],"metadata":{"id":"rKes7C2Cf0_F"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm.auto import tqdm\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import umap\n","\n","from Bio import SeqIO\n","from Bio.Seq import Seq\n","from Bio.SeqRecord import SeqRecord\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seeds for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","print(\"✓ Libraries imported\")"],"metadata":{"id":"QZ-CPMaEf5rk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 3: DNA Encoding Utilities"],"metadata":{"id":"I1MHyLnNgAGs"}},{"cell_type":"code","source":["class DNAEncoder:\n","    \"\"\"\n","    Convert DNA sequences to numerical representations.\n","    Supports one-hot encoding with proper handling of ambiguous bases.\n","    \"\"\"\n","\n","    @staticmethod\n","    def one_hot_encode(sequence):\n","        \"\"\"\n","        One-hot encoding: A=[1,0,0,0], C=[0,1,0,0], G=[0,0,1,0], T=[0,0,0,1]\n","        Returns: (4, seq_length) array\n","        \"\"\"\n","        mapping = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n","        seq_upper = sequence.upper()\n","\n","        encoded = np.zeros((4, len(seq_upper)), dtype=np.float32)\n","\n","        for idx, nucleotide in enumerate(seq_upper):\n","            if nucleotide in mapping:\n","                encoded[mapping[nucleotide], idx] = 1.0\n","            # Ambiguous bases (N, etc.) result in all-zero columns\n","\n","        return encoded\n","\n","    @staticmethod\n","    def decode_one_hot(encoded_array):\n","        \"\"\"\n","        Convert one-hot encoded array back to DNA sequence.\n","        Args:\n","            encoded_array: (4, seq_length) array\n","        Returns:\n","            DNA sequence string\n","        \"\"\"\n","        bases = 'ACGT'\n","        sequence = ''.join([bases[np.argmax(encoded_array[:, i])]\n","                           for i in range(encoded_array.shape[1])])\n","        return sequence\n","\n","\n","# Test the encoder\n","test_seq = \"ATCGATCGATCGNNNATCG\"\n","encoded = DNAEncoder.one_hot_encode(test_seq)\n","decoded = DNAEncoder.decode_one_hot(encoded)\n","\n","print(f\"Original:  {test_seq}\")\n","print(f\"Decoded:   {decoded}\")\n","print(f\"Encoding shape: {encoded.shape}\")\n","print(\"✓ DNA encoder working\")"],"metadata":{"id":"Tb_9IYt4gLng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 4: Dataset Class"],"metadata":{"id":"O478vRO6gN2e"}},{"cell_type":"code","source":["class GenomicDataset(Dataset):\n","    \"\"\"\n","    PyTorch Dataset for genomic sequences.\n","    Extracts fixed-length windows from FASTA files.\n","    \"\"\"\n","\n","    def __init__(self, fasta_file, window_size=1024, stride=512,\n","                 max_samples=None, filter_n_threshold=0.1):\n","        \"\"\"\n","        Args:\n","            fasta_file: Path to FASTA file\n","            window_size: Length of sequence windows\n","            stride: Sliding window stride\n","            max_samples: Maximum number of samples to extract (None = all)\n","            filter_n_threshold: Maximum proportion of N bases allowed\n","        \"\"\"\n","        self.window_size = window_size\n","        self.stride = stride\n","        self.sequences = []\n","\n","        print(f\"Loading sequences from {fasta_file}...\")\n","\n","        for record in SeqIO.parse(fasta_file, \"fasta\"):\n","            sequence = str(record.seq).upper()\n","\n","            # Sliding window extraction\n","            for i in range(0, len(sequence) - window_size + 1, stride):\n","                if max_samples and len(self.sequences) >= max_samples:\n","                    break\n","\n","                chunk = sequence[i:i + window_size]\n","\n","                # Filter sequences with too many ambiguous bases\n","                n_count = chunk.count('N')\n","                if n_count / len(chunk) <= filter_n_threshold:\n","                    self.sequences.append(chunk)\n","\n","            if max_samples and len(self.sequences) >= max_samples:\n","                break\n","\n","        print(f\"✓ Created dataset: {len(self.sequences)} sequences of {window_size} bp\")\n","        print(f\"  Overlap: {window_size - stride} bp\")\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        sequence = self.sequences[idx]\n","\n","        # One-hot encode\n","        encoded = DNAEncoder.one_hot_encode(sequence)\n","\n","        # Flatten: (4, 1024) -> (4096,)\n","        encoded_flat = encoded.flatten()\n","\n","        return torch.tensor(encoded_flat, dtype=torch.float32)\n","\n","    def get_sequence(self, idx):\n","        \"\"\"Return raw sequence string\"\"\"\n","        return self.sequences[idx]\n","\n","\n","def create_synthetic_genome(length=5_000_000, output_file='synthetic_genome.fasta'):\n","    \"\"\"\n","    Generate synthetic genome with realistic base composition.\n","    C. elegans has ~36% GC content.\n","    \"\"\"\n","    # Weighted base selection (approximating C. elegans)\n","    bases = ['A', 'T', 'G', 'C']\n","    weights = [0.32, 0.32, 0.18, 0.18]  # ~36% GC\n","\n","    sequence = ''.join(np.random.choice(bases, size=length, p=weights))\n","\n","    record = SeqRecord(\n","        Seq(sequence),\n","        id=\"synthetic_chr\",\n","        description=f\"Synthetic {length/1e6:.1f}Mb genome for testing\"\n","    )\n","\n","    SeqIO.write(record, output_file, \"fasta\")\n","    print(f\"✓ Created synthetic genome: {output_file} ({length/1e6:.1f} Mb)\")\n","\n","    return output_file\n","\n","\n","# Create synthetic data for testing\n","synthetic_file = create_synthetic_genome(length=5_000_000)  # 5 Mb\n","\n","# Create dataset\n","dataset = GenomicDataset(\n","    fasta_file=synthetic_file,\n","    window_size=1024,\n","    stride=512,\n","    max_samples=100_000  # Limit to 100k samples for faster training\n",")\n","\n","print(f\"\\nSample shape: {dataset[0].shape}\")\n","print(f\"Sample stats - Min: {dataset[0].min():.2f}, Max: {dataset[0].max():.2f}\")"],"metadata":{"id":"S6N-qViKgS-T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 5: Hierarchical VAE Architecture"],"metadata":{"id":"tELYKKPCgbzt"}},{"cell_type":"code","source":["class HierarchicalVAE(nn.Module):\n","    \"\"\"\n","    Multi-scale Variational Autoencoder with hierarchical latent spaces.\n","\n","    Architecture:\n","        Input (4096) -> Encoder -> 3 latent spaces [256, 512, 1024]\n","        Latent spaces -> Decoder -> Reconstruction (4096)\n","\n","    The model learns to represent data at multiple levels of abstraction:\n","        - Level 1 (256d): Most abstract, compressed representation\n","        - Level 2 (512d): Intermediate features\n","        - Level 3 (1024d): Fine-grained details\n","    \"\"\"\n","\n","    def __init__(self, input_dim=4096, latent_dims=[256, 512, 1024], dropout=0.3):\n","        super().__init__()\n","\n","        self.input_dim = input_dim\n","        self.latent_dims = latent_dims\n","\n","        # ========================\n","        # ENCODER PATHWAY\n","        # ========================\n","\n","        # Stage 1: Input -> 2048\n","        self.enc1 = nn.Sequential(\n","            nn.Linear(input_dim, 2048),\n","            nn.LayerNorm(2048),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Stage 2: 2048 -> 1024\n","        self.enc2 = nn.Sequential(\n","            nn.Linear(2048, 1024),\n","            nn.LayerNorm(1024),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Stage 3: 1024 -> 512 (deepest layer)\n","        self.enc3 = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # ========================\n","        # LATENT SPACE PROJECTIONS\n","        # ========================\n","\n","        # Latent level 1: From deepest layer (most abstract)\n","        self.z1_mu = nn.Linear(512, latent_dims[0])\n","        self.z1_logvar = nn.Linear(512, latent_dims[0])\n","\n","        # Latent level 2: From intermediate layer\n","        self.z2_mu = nn.Linear(1024, latent_dims[1])\n","        self.z2_logvar = nn.Linear(1024, latent_dims[1])\n","\n","        # Latent level 3: From shallow layer (fine details)\n","        self.z3_mu = nn.Linear(2048, latent_dims[2])\n","        self.z3_logvar = nn.Linear(2048, latent_dims[2])\n","\n","        # ========================\n","        # DECODER PATHWAY\n","        # ========================\n","\n","        total_latent_dim = sum(latent_dims)  # 256 + 512 + 1024 = 1792\n","\n","        # Stage 1: Concatenated latents -> 512\n","        self.dec1 = nn.Sequential(\n","            nn.Linear(total_latent_dim, 512),\n","            nn.LayerNorm(512),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Stage 2: 512 -> 1024\n","        self.dec2 = nn.Sequential(\n","            nn.Linear(512, 1024),\n","            nn.LayerNorm(1024),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Stage 3: 1024 -> 2048\n","        self.dec3 = nn.Sequential(\n","            nn.Linear(1024, 2048),\n","            nn.LayerNorm(2048),\n","            nn.GELU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        # Output layer: 2048 -> 4096 (reconstruction)\n","        self.output = nn.Linear(2048, input_dim)\n","\n","        # Initialize weights\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        \"\"\"Xavier initialization for better gradient flow\"\"\"\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.xavier_uniform_(module.weight)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","    def reparameterize(self, mu, logvar):\n","        \"\"\"\n","        Reparameterization trick: z = mu + std * epsilon\n","        Allows gradients to flow through sampling operation.\n","        \"\"\"\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def encode(self, x):\n","        \"\"\"\n","        Encode input into hierarchical latent representations.\n","\n","        Returns:\n","            latents: Tuple of (z1, z2, z3) sampled latent vectors\n","            params: List of (mu, logvar) tuples for KL divergence calculation\n","        \"\"\"\n","        # Forward through encoder stages\n","        h1 = self.enc1(x)      # (batch, 2048)\n","        h2 = self.enc2(h1)     # (batch, 1024)\n","        h3 = self.enc3(h2)     # (batch, 512)\n","\n","        # Extract latent parameters from each level\n","        # Level 1: Most abstract (from deepest layer)\n","        z1_mu = self.z1_mu(h3)\n","        z1_logvar = self.z1_logvar(h3)\n","        z1 = self.reparameterize(z1_mu, z1_logvar)\n","\n","        # Level 2: Intermediate\n","        z2_mu = self.z2_mu(h2)\n","        z2_logvar = self.z2_logvar(h2)\n","        z2 = self.reparameterize(z2_mu, z2_logvar)\n","\n","        # Level 3: Fine details (from shallowest layer)\n","        z3_mu = self.z3_mu(h1)\n","        z3_logvar = self.z3_logvar(h1)\n","        z3 = self.reparameterize(z3_mu, z3_logvar)\n","\n","        latents = (z1, z2, z3)\n","        params = [(z1_mu, z1_logvar), (z2_mu, z2_logvar), (z3_mu, z3_logvar)]\n","\n","        return latents, params\n","\n","    def decode(self, latents):\n","        \"\"\"\n","        Decode from hierarchical latent space to reconstruction.\n","\n","        Args:\n","            latents: Tuple of (z1, z2, z3)\n","        \"\"\"\n","        # Concatenate all latent levels\n","        z = torch.cat(latents, dim=-1)  # (batch, 1792)\n","\n","        # Decode through stages\n","        h = self.dec1(z)\n","        h = self.dec2(h)\n","        h = self.dec3(h)\n","\n","        return self.output(h)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Full forward pass: encode -> sample -> decode\n","\n","        Returns:\n","            reconstruction: Reconstructed input\n","            latents: Sampled latent vectors\n","            params: Distribution parameters for loss calculation\n","        \"\"\"\n","        latents, params = self.encode(x)\n","        reconstruction = self.decode(latents)\n","\n","        return reconstruction, latents, params\n","\n","\n","# Instantiate model\n","model = HierarchicalVAE(\n","    input_dim=4096,\n","    latent_dims=[256, 512, 1024],\n","    dropout=0.3\n",")\n","\n","# Count parameters\n","total_params = sum(p.numel() for p in model.parameters())\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f\"✓ Model created\")\n","print(f\"  Total parameters: {total_params:,}\")\n","print(f\"  Trainable parameters: {trainable_params:,}\")\n","print(f\"  Latent space dimensions: {model.latent_dims}\")\n","print(f\"  Total latent dimension: {sum(model.latent_dims)}\")"],"metadata":{"id":"v2gNHiDHgfwA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 6: Loss Functions"],"metadata":{"id":"rMIIf_F4gqP2"}},{"cell_type":"code","source":["def vae_loss_function(recon_x, x, latent_params, beta=1.0, kl_weights=[1.0, 1.0, 1.0]):\n","    \"\"\"\n","    VAE loss = Reconstruction loss + β * KL divergence\n","\n","    Args:\n","        recon_x: Reconstructed input\n","        x: Original input\n","        latent_params: List of (mu, logvar) tuples for each latent level\n","        beta: KL divergence weighting factor (β-VAE)\n","        kl_weights: Per-level KL weights (for hierarchical control)\n","\n","    Returns:\n","        total_loss: Combined loss\n","        recon_loss: Reconstruction term\n","        kl_loss: KL divergence term\n","        kl_per_level: KL divergence for each hierarchical level\n","    \"\"\"\n","    # Reconstruction loss (MSE)\n","    recon_loss = F.mse_loss(recon_x, x, reduction='sum') / x.size(0)\n","\n","    # KL divergence for each latent level\n","    kl_per_level = []\n","    kl_loss = 0\n","\n","    for idx, (mu, logvar) in enumerate(latent_params):\n","        # KL(N(mu, sigma) || N(0, 1))\n","        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n","        kl = kl.mean()  # Average over batch\n","\n","        kl_per_level.append(kl.item())\n","        kl_loss += kl_weights[idx] * kl\n","\n","    # Total loss\n","    total_loss = recon_loss + beta * kl_loss\n","\n","    return total_loss, recon_loss, kl_loss, kl_per_level\n","\n","\n","print(\"✓ Loss function defined\")"],"metadata":{"id":"5dKzMuVHgtAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 7: Training Loop"],"metadata":{"id":"sxU7-hJmgzv8"}},{"cell_type":"code","source":["def train_hierarchical_vae(model, train_loader, val_loader,\n","                          epochs=100, lr=1e-3,\n","                          beta_schedule=None,\n","                          device='cuda'):\n","    \"\"\"\n","    Training loop with β-annealing and comprehensive monitoring.\n","\n","    Args:\n","        beta_schedule: Function that returns beta value given epoch number\n","                      If None, uses constant β=1.0\n","    \"\"\"\n","    model.to(device)\n","\n","    # Optimizer with weight decay for regularization\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n","\n","    # Learning rate scheduler: Cosine annealing with warm restarts\n","    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","        optimizer, T_0=10, T_mult=2, eta_min=1e-6\n","    )\n","\n","    # Training history\n","    history = {\n","        'train_loss': [], 'train_recon': [], 'train_kl': [],\n","        'val_loss': [], 'val_recon': [], 'val_kl': [],\n","        'kl_level1': [], 'kl_level2': [], 'kl_level3': [],\n","        'beta_values': []\n","    }\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    patience = 15\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Starting training on {device}\")\n","    print(f\"{'='*60}\\n\")\n","\n","    for epoch in range(epochs):\n","        # Determine β value for this epoch\n","        if beta_schedule is not None:\n","            beta = beta_schedule(epoch)\n","        else:\n","            beta = 1.0\n","\n","        history['beta_values'].append(beta)\n","\n","        # ==================\n","        # TRAINING PHASE\n","        # ==================\n","        model.train()\n","        train_loss = 0\n","        train_recon = 0\n","        train_kl = 0\n","        kl_levels = [0, 0, 0]\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n","\n","        for batch_idx, batch in enumerate(pbar):\n","            x = batch.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            recon, latents, params = model(x)\n","\n","            # Compute loss\n","            loss, recon_loss, kl_loss, kl_per_level = vae_loss_function(\n","                recon, x, params, beta=beta\n","            )\n","\n","            # Backward pass\n","            loss.backward()\n","\n","            # Gradient clipping\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","            optimizer.step()\n","\n","            # Accumulate metrics\n","            train_loss += loss.item()\n","            train_recon += recon_loss.item()\n","            train_kl += kl_loss.item()\n","            for i in range(3):\n","                kl_levels[i] += kl_per_level[i]\n","\n","            # Update progress bar\n","            pbar.set_postfix({\n","                'loss': f'{loss.item():.4f}',\n","                'recon': f'{recon_loss.item():.4f}',\n","                'kl': f'{kl_loss.item():.4f}',\n","                'β': f'{beta:.3f}'\n","            })\n","\n","        # Average training metrics\n","        n_train = len(train_loader)\n","        avg_train_loss = train_loss / n_train\n","        avg_train_recon = train_recon / n_train\n","        avg_train_kl = train_kl / n_train\n","        avg_kl_levels = [kl / n_train for kl in kl_levels]\n","\n","        # ==================\n","        # VALIDATION PHASE\n","        # ==================\n","        model.eval()\n","        val_loss = 0\n","        val_recon = 0\n","        val_kl = 0\n","\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                x = batch.to(device)\n","\n","                recon, latents, params = model(x)\n","                loss, recon_loss, kl_loss, _ = vae_loss_function(\n","                    recon, x, params, beta=beta\n","                )\n","\n","                val_loss += loss.item()\n","                val_recon += recon_loss.item()\n","                val_kl += kl_loss.item()\n","\n","        # Average validation metrics\n","        n_val = len(val_loader)\n","        avg_val_loss = val_loss / n_val\n","        avg_val_recon = val_recon / n_val\n","        avg_val_kl = val_kl / n_val\n","\n","        # Update history\n","        history['train_loss'].append(avg_train_loss)\n","        history['train_recon'].append(avg_train_recon)\n","        history['train_kl'].append(avg_train_kl)\n","        history['val_loss'].append(avg_val_loss)\n","        history['val_recon'].append(avg_val_recon)\n","        history['val_kl'].append(avg_val_kl)\n","        history['kl_level1'].append(avg_kl_levels[0])\n","        history['kl_level2'].append(avg_kl_levels[1])\n","        history['kl_level3'].append(avg_kl_levels[2])\n","\n","        # Learning rate scheduling\n","        scheduler.step()\n","        current_lr = optimizer.param_groups[0]['lr']\n","\n","        # Print epoch summary\n","        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n","        print(f\"  Train Loss: {avg_train_loss:.4f} | Recon: {avg_train_recon:.4f} | KL: {avg_train_kl:.4f}\")\n","        print(f\"  Val Loss:   {avg_val_loss:.4f} | Recon: {avg_val_recon:.4f} | KL: {avg_val_kl:.4f}\")\n","        print(f\"  KL Levels:  L1={avg_kl_levels[0]:.2f} | L2={avg_kl_levels[1]:.2f} | L3={avg_kl_levels[2]:.2f}\")\n","        print(f\"  LR: {current_lr:.2e} | β: {beta:.3f}\")\n","\n","        # Early stopping check\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","\n","            # Save best model\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'train_loss': avg_train_loss,\n","                'val_loss': avg_val_loss,\n","                'history': history\n","            }, 'best_hierarchical_vae.pth')\n","\n","            print(f\"  ✓ Best model saved (val_loss: {best_val_loss:.4f})\")\n","        else:\n","            patience_counter += 1\n","            print(f\"  Patience: {patience_counter}/{patience}\")\n","\n","        if patience_counter >= patience:\n","            print(f\"\\n{'='*60}\")\n","            print(f\"Early stopping triggered at epoch {epoch+1}\")\n","            print(f\"{'='*60}\\n\")\n","            break\n","\n","    print(\"\\n✓ Training complete\")\n","    return history\n","\n","\n","print(\"✓ Training function defined\")"],"metadata":{"id":"u2DspUrRg7U6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 8: β-Annealing Schedule"],"metadata":{"id":"gqUIVZXLhASe"}},{"cell_type":"code","source":["def beta_annealing_schedule(epoch, warmup_epochs=20, max_beta=1.0, mode='linear'):\n","    \"\"\"\n","    β-annealing for VAE training.\n","    Start with β=0 (pure autoencoder) and gradually increase.\n","\n","    Modes:\n","        'linear': Linear increase from 0 to max_beta\n","        'cyclical': Cyclical annealing (multiple cycles)\n","        'constant': No annealing, use max_beta throughout\n","    \"\"\"\n","    if mode == 'constant':\n","        return max_beta\n","\n","    elif mode == 'linear':\n","        if epoch < warmup_epochs:\n","            return (epoch / warmup_epochs) * max_beta\n","        return max_beta\n","\n","    elif mode == 'cyclical':\n","        cycle_length = 20\n","        cycle_progress = (epoch % cycle_length) / cycle_length\n","        return cycle_progress * max_beta\n","\n","    return max_beta\n","\n","\n","# Test the schedule\n","epochs_test = 50\n","betas = [beta_annealing_schedule(e, warmup_epochs=20, mode='linear') for e in range(epochs_test)]\n","\n","plt.figure(figsize=(10, 4))\n","plt.plot(betas, linewidth=2)\n","plt.xlabel('Epoch')\n","plt.ylabel('β value')\n","plt.title('β-Annealing Schedule (Linear Warmup)')\n","plt.grid(alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"✓ β-annealing schedule defined\")"],"metadata":{"id":"1Qiye9Vtg_-V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 9: Create Data Loaders and Train"],"metadata":{"id":"gsB6l6M8hPWi"}},{"cell_type":"code","source":["# Split dataset\n","train_size = int(0.8 * len(dataset))\n","val_size = int(0.1 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","train_dataset, val_dataset, test_dataset = random_split(\n","    dataset,\n","    [train_size, val_size, test_size],\n","    generator=torch.Generator().manual_seed(42)\n",")\n","\n","# Create data loaders\n","batch_size = 128\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n","                         num_workers=2, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n","                       num_workers=2, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n","                        num_workers=2, pin_memory=True)\n","\n","print(f\"Dataset splits:\")\n","print(f\"  Train: {len(train_dataset):,} samples\")\n","print(f\"  Val:   {len(val_dataset):,} samples\")\n","print(f\"  Test:  {len(test_dataset):,} samples\")\n","print(f\"  Batch size: {batch_size}\")\n","print(f\"  Batches per epoch: {len(train_loader):,}\\n\")\n","\n","# Train the model\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","history = train_hierarchical_vae(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    epochs=100,\n","    lr=1e-3,\n","    beta_schedule=lambda epoch: beta_annealing_schedule(epoch, warmup_epochs=20, mode='linear'),\n","    device=device\n",")"],"metadata":{"id":"rCx4JhiLhXvi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2: Latent Space Analysis & Visualization section"],"metadata":{"id":"ilWQerbbUggm"}},{"cell_type":"markdown","source":["Cell 10: Load Best Model"],"metadata":{"id":"Cjm7sooLhuvw"}},{"cell_type":"code","source":["# Load the best checkpoint\n","checkpoint = torch.load('best_hierarchical_vae.pth')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)\n","model.eval()\n","\n","print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n","print(f\"  Final validation loss: {checkpoint['val_loss']:.4f}\")"],"metadata":{"id":"V2WEeWWgUvSa","executionInfo":{"status":"ok","timestamp":1763391604953,"user_tz":0,"elapsed":5,"user":{"displayName":"Michael Matley","userId":"05958962179016968211"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 11: Training History Visualization"],"metadata":{"id":"rpEPoZWLUv8J"}},{"cell_type":"code","source":["def plot_training_history(history):\n","    \"\"\"\n","    Comprehensive visualization of training dynamics.\n","    \"\"\"\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # Plot 1: Total Loss\n","    ax = axes[0, 0]\n","    ax.plot(history['train_loss'], label='Train Loss', linewidth=2, alpha=0.8)\n","    ax.plot(history['val_loss'], label='Val Loss', linewidth=2, alpha=0.8)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Loss')\n","    ax.set_title('Total Loss (Reconstruction + KL)', fontsize=12, fontweight='bold')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    # Plot 2: Reconstruction Loss\n","    ax = axes[0, 1]\n","    ax.plot(history['train_recon'], label='Train Recon', linewidth=2, alpha=0.8)\n","    ax.plot(history['val_recon'], label='Val Recon', linewidth=2, alpha=0.8)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('Reconstruction Loss')\n","    ax.set_title('Reconstruction Loss (MSE)', fontsize=12, fontweight='bold')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    # Plot 3: KL Divergence\n","    ax = axes[1, 0]\n","    ax.plot(history['train_kl'], label='Train KL', linewidth=2, alpha=0.8, color='crimson')\n","    ax.plot(history['val_kl'], label='Val KL', linewidth=2, alpha=0.8, color='darkred')\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('KL Divergence')\n","    ax.set_title('KL Divergence', fontsize=12, fontweight='bold')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    # Plot 4: Hierarchical KL Levels\n","    ax = axes[1, 1]\n","    ax.plot(history['kl_level1'], label='Level 1 (256d)', linewidth=2, alpha=0.8)\n","    ax.plot(history['kl_level2'], label='Level 2 (512d)', linewidth=2, alpha=0.8)\n","    ax.plot(history['kl_level3'], label='Level 3 (1024d)', linewidth=2, alpha=0.8)\n","    ax.set_xlabel('Epoch')\n","    ax.set_ylabel('KL Divergence')\n","    ax.set_title('KL Divergence by Hierarchical Level', fontsize=12, fontweight='bold')\n","    ax.legend()\n","    ax.grid(alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"✓ Training history visualized\")\n","\n","\n","plot_training_history(history)"],"metadata":{"id":"HHsLrmyIVT04"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 12: Extract Latent Representations"],"metadata":{"id":"7M8OiVUtVY0f"}},{"cell_type":"code","source":["def extract_latent_representations(model, dataloader, device, max_samples=10000):\n","    \"\"\"\n","    Extract all three hierarchical latent levels from the model.\n","\n","    Returns:\n","        latents_dict: Dictionary with keys 'level1', 'level2', 'level3'\n","        Each contains numpy array of shape (num_samples, latent_dim)\n","    \"\"\"\n","    model.eval()\n","\n","    latents_l1 = []\n","    latents_l2 = []\n","    latents_l3 = []\n","\n","    samples_processed = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Extracting latents\"):\n","            if samples_processed >= max_samples:\n","                break\n","\n","            x = batch.to(device)\n","\n","            # Get latent representations\n","            _, latents, _ = model(x)\n","            z1, z2, z3 = latents\n","\n","            latents_l1.append(z1.cpu().numpy())\n","            latents_l2.append(z2.cpu().numpy())\n","            latents_l3.append(z3.cpu().numpy())\n","\n","            samples_processed += len(x)\n","\n","    # Concatenate all batches\n","    latents_dict = {\n","        'level1': np.concatenate(latents_l1, axis=0)[:max_samples],\n","        'level2': np.concatenate(latents_l2, axis=0)[:max_samples],\n","        'level3': np.concatenate(latents_l3, axis=0)[:max_samples]\n","    }\n","\n","    print(f\"\\n✓ Extracted latent representations:\")\n","    for level, latents in latents_dict.items():\n","        print(f\"  {level}: {latents.shape}\")\n","\n","    return latents_dict\n","\n","\n","# Extract latents from test set\n","latents_dict = extract_latent_representations(model, test_loader, device, max_samples=10000)"],"metadata":{"id":"eMsllW0UVbet"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 13: Intrinsic Dimensionality Analysis"],"metadata":{"id":"UYjCYqNaVhOp"}},{"cell_type":"code","source":["def analyze_intrinsic_dimensionality(latents_dict):\n","    \"\"\"\n","    Measure the intrinsic dimensionality of each latent level using PCA.\n","\n","    Intrinsic dimensionality = number of components needed to explain 95% variance.\n","    This tells us how much the model actually uses its latent capacity.\n","    \"\"\"\n","    results = {}\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    for idx, (level_name, latents) in enumerate(latents_dict.items()):\n","        # Fit PCA\n","        pca = PCA()\n","        pca.fit(latents)\n","\n","        # Calculate cumulative explained variance\n","        cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n","\n","        # Find intrinsic dimensionality (95% threshold)\n","        intrinsic_dim = np.argmax(cumsum_variance >= 0.95) + 1\n","\n","        # Store results\n","        results[level_name] = {\n","            'nominal_dim': latents.shape[1],\n","            'intrinsic_dim': intrinsic_dim,\n","            'explained_variance_ratio': pca.explained_variance_ratio_,\n","            'cumsum_variance': cumsum_variance\n","        }\n","\n","        # Plot\n","        ax = axes[idx]\n","        ax.plot(cumsum_variance, linewidth=2.5, color='darkblue')\n","        ax.axhline(y=0.95, color='red', linestyle='--', linewidth=2, alpha=0.7, label='95% threshold')\n","        ax.axvline(x=intrinsic_dim, color='green', linestyle='--', linewidth=2, alpha=0.7,\n","                   label=f'Intrinsic dim: {intrinsic_dim}')\n","        ax.set_xlabel('Number of Components', fontsize=11)\n","        ax.set_ylabel('Cumulative Explained Variance', fontsize=11)\n","        ax.set_title(f'{level_name.capitalize()} ({latents.shape[1]}d)',\n","                     fontsize=12, fontweight='bold')\n","        ax.legend(fontsize=10)\n","        ax.grid(alpha=0.3)\n","        ax.set_ylim([0, 1.05])\n","\n","    plt.tight_layout()\n","    plt.savefig('intrinsic_dimensionality.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    # Print summary\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"INTRINSIC DIMENSIONALITY ANALYSIS\")\n","    print(\"=\"*60)\n","    for level_name, result in results.items():\n","        utilization = (result['intrinsic_dim'] / result['nominal_dim']) * 100\n","        print(f\"\\n{level_name.upper()}:\")\n","        print(f\"  Nominal dimension:    {result['nominal_dim']}\")\n","        print(f\"  Intrinsic dimension:  {result['intrinsic_dim']}\")\n","        print(f\"  Utilization:          {utilization:.1f}%\")\n","    print(\"=\"*60)\n","\n","    return results\n","\n","\n","intrinsic_dims = analyze_intrinsic_dimensionality(latents_dict)"],"metadata":{"id":"FCx84X0lVlOz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 14: Latent Space Visualization with UMAP"],"metadata":{"id":"y1aqkLA7VpWx"}},{"cell_type":"code","source":["def visualize_latent_space_umap(latents_dict, n_samples=5000):\n","    \"\"\"\n","    Visualize all three latent levels using UMAP dimensionality reduction.\n","    UMAP preserves both local and global structure.\n","    \"\"\"\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    for idx, (level_name, latents) in enumerate(latents_dict.items()):\n","        # Subsample for faster computation\n","        if len(latents) > n_samples:\n","            indices = np.random.choice(len(latents), n_samples, replace=False)\n","            latents_subset = latents[indices]\n","        else:\n","            latents_subset = latents\n","\n","        print(f\"Computing UMAP for {level_name}...\")\n","\n","        # Fit UMAP\n","        reducer = umap.UMAP(\n","            n_components=2,\n","            n_neighbors=15,\n","            min_dist=0.1,\n","            metric='euclidean',\n","            random_state=42\n","        )\n","        embedding = reducer.fit_transform(latents_subset)\n","\n","        # Plot\n","        ax = axes[idx]\n","        scatter = ax.scatter(\n","            embedding[:, 0],\n","            embedding[:, 1],\n","            c=np.arange(len(embedding)),  # Color by index (temporal ordering)\n","            cmap='viridis',\n","            s=10,\n","            alpha=0.6,\n","            rasterized=True\n","        )\n","\n","        ax.set_xlabel('UMAP 1', fontsize=11)\n","        ax.set_ylabel('UMAP 2', fontsize=11)\n","        ax.set_title(f'{level_name.capitalize()} ({latents.shape[1]}d → 2d)',\n","                     fontsize=12, fontweight='bold')\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","        # Add colorbar\n","        cbar = plt.colorbar(scatter, ax=ax)\n","        cbar.set_label('Sample Index', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.savefig('latent_space_umap.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"✓ UMAP visualization complete\")\n","\n","\n","visualize_latent_space_umap(latents_dict, n_samples=5000)"],"metadata":{"id":"vdlxuaczVvQo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 15: t-SNE Visualization (Alternative)"],"metadata":{"id":"LNxnRDDPVy6J"}},{"cell_type":"code","source":["visualize_latent_space_tsne(latents_dict, n_samples=3000):\n","    \"\"\"\n","    Visualize using t-SNE (preserves local structure better than global).\n","    \"\"\"\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    for idx, (level_name, latents) in enumerate(latents_dict.items()):\n","        # Subsample for faster computation\n","        if len(latents) > n_samples:\n","            indices = np.random.choice(len(latents), n_samples, replace=False)\n","            latents_subset = latents[indices]\n","        else:\n","            latents_subset = latents\n","\n","        print(f\"Computing t-SNE for {level_name}...\")\n","\n","        # Fit t-SNE\n","        tsne = TSNE(\n","            n_components=2,\n","            perplexity=30,\n","            n_iter=1000,\n","            random_state=42\n","        )\n","        embedding = tsne.fit_transform(latents_subset)\n","\n","        # Plot\n","        ax = axes[idx]\n","        scatter = ax.scatter(\n","            embedding[:, 0],\n","            embedding[:, 1],\n","            c=np.arange(len(embedding)),\n","            cmap='plasma',\n","            s=10,\n","            alpha=0.6,\n","            rasterized=True\n","        )\n","\n","        ax.set_xlabel('t-SNE 1', fontsize=11)\n","        ax.set_ylabel('t-SNE 2', fontsize=11)\n","        ax.set_title(f'{level_name.capitalize()} t-SNE',\n","                     fontsize=12, fontweight='bold')\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","        cbar = plt.colorbar(scatter, ax=ax)\n","        cbar.set_label('Sample Index', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.savefig('latent_space_tsne.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"✓ t-SNE visualization complete\")\n","\n","\n","visualize_latent_space_tsne(latents_dict, n_samples=3000)"],"metadata":{"id":"FI0Ut-CZV7Ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 16: Reconstruction Quality Assessment"],"metadata":{"id":"_p2Mj_5LV-UC"}},{"cell_type":"code","source":["def evaluate_reconstruction_quality(model, dataloader, device, num_samples=10):\n","    \"\"\"\n","    Evaluate how well the model reconstructs sequences.\n","    Shows per-nucleotide accuracy and visualizes differences.\n","    \"\"\"\n","    model.eval()\n","\n","    samples_shown = 0\n","    all_accuracies = []\n","\n","    fig, axes = plt.subplots(num_samples, 1, figsize=(16, num_samples * 1.5))\n","    if num_samples == 1:\n","        axes = [axes]\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            if samples_shown >= num_samples:\n","                break\n","\n","            x = batch.to(device)\n","\n","            # Get reconstruction\n","            recon, _, _ = model(x)\n","\n","            for i in range(min(len(x), num_samples - samples_shown)):\n","                # Convert to numpy\n","                original = x[i].cpu().numpy().reshape(4, 1024)\n","                reconstructed = recon[i].cpu().numpy().reshape(4, 1024)\n","\n","                # Decode to sequences\n","                orig_seq = DNAEncoder.decode_one_hot(original)\n","                recon_seq = DNAEncoder.decode_one_hot(reconstructed)\n","\n","                # Calculate per-base accuracy\n","                matches = [1 if o == r else 0 for o, r in zip(orig_seq, recon_seq)]\n","                accuracy = sum(matches) / len(matches)\n","                all_accuracies.append(accuracy)\n","\n","                # Visualize alignment\n","                ax = axes[samples_shown]\n","\n","                # Show first 100 bases\n","                display_length = 100\n","                orig_display = orig_seq[:display_length]\n","                recon_display = recon_seq[:display_length]\n","                matches_display = matches[:display_length]\n","\n","                # Create visualization\n","                colors = ['red' if m == 0 else 'green' for m in matches_display]\n","\n","                for pos, (o, r, color) in enumerate(zip(orig_display, recon_display, colors)):\n","                    ax.text(pos, 1, o, ha='center', va='center', fontsize=8,\n","                           family='monospace', color='black')\n","                    ax.text(pos, 0, r, ha='center', va='center', fontsize=8,\n","                           family='monospace', color=color, fontweight='bold')\n","\n","                ax.set_xlim(-1, display_length)\n","                ax.set_ylim(-0.5, 1.5)\n","                ax.set_yticks([0, 1])\n","                ax.set_yticklabels(['Recon', 'Original'], fontsize=9)\n","                ax.set_title(f'Sample {samples_shown+1} | Accuracy: {accuracy:.2%} | '\n","                           f'Errors: {sum(1 for m in matches if m == 0)}/{len(matches)}',\n","                           fontsize=10, fontweight='bold', loc='left')\n","                ax.set_xticks([])\n","                ax.spines['top'].set_visible(False)\n","                ax.spines['right'].set_visible(False)\n","                ax.spines['bottom'].set_visible(False)\n","\n","                samples_shown += 1\n","\n","                if samples_shown >= num_samples:\n","                    break\n","\n","    plt.tight_layout()\n","    plt.savefig('reconstruction_quality.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    # Print statistics\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"RECONSTRUCTION QUALITY STATISTICS\")\n","    print(\"=\"*60)\n","    print(f\"Mean accuracy:   {np.mean(all_accuracies):.4f}\")\n","    print(f\"Median accuracy: {np.median(all_accuracies):.4f}\")\n","    print(f\"Std deviation:   {np.std(all_accuracies):.4f}\")\n","    print(f\"Min accuracy:    {np.min(all_accuracies):.4f}\")\n","    print(f\"Max accuracy:    {np.max(all_accuracies):.4f}\")\n","    print(\"=\"*60)\n","\n","    return all_accuracies\n","\n","\n","reconstruction_accuracies = evaluate_reconstruction_quality(\n","    model, test_loader, device, num_samples=10\n",")"],"metadata":{"id":"9o1AF-QEWCQN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 17: Latent Space Interpolation"],"metadata":{"id":"hAPu2Z10WF9L"}},{"cell_type":"code","source":["def interpolate_latent_space(model, dataloader, device, num_steps=10):\n","    \"\"\"\n","    Interpolate between two random points in latent space.\n","    Shows what the model has learned about smooth transitions.\n","    \"\"\"\n","    model.eval()\n","\n","    # Get two random samples\n","    batch = next(iter(dataloader)).to(device)\n","    x1, x2 = batch[0:1], batch[1:2]\n","\n","    with torch.no_grad():\n","        # Encode to latent space\n","        latents1, _ = model.encode(x1)\n","        latents2, _ = model.encode(x2)\n","\n","        # Interpolate at each hierarchical level\n","        interpolations = []\n","\n","        for alpha in np.linspace(0, 1, num_steps):\n","            interp_latents = tuple(\n","                (1 - alpha) * z1 + alpha * z2\n","                for z1, z2 in zip(latents1, latents2)\n","            )\n","\n","            # Decode\n","            recon = model.decode(interp_latents)\n","            interpolations.append(recon.cpu().numpy())\n","\n","        interpolations = np.array(interpolations)\n","\n","    # Visualize\n","    fig, axes = plt.subplots(num_steps, 1, figsize=(16, num_steps * 1))\n","\n","    for idx, interp in enumerate(interpolations):\n","        # Decode to sequence\n","        interp_reshaped = interp[0].reshape(4, 1024)\n","        seq = DNAEncoder.decode_one_hot(interp_reshaped)\n","\n","        # Show first 100 bases\n","        display_seq = seq[:100]\n","\n","        ax = axes[idx]\n","        for pos, base in enumerate(display_seq):\n","            color_map = {'A': 'green', 'C': 'blue', 'G': 'orange', 'T': 'red'}\n","            ax.text(pos, 0, base, ha='center', va='center', fontsize=8,\n","                   family='monospace', color=color_map.get(base, 'black'))\n","\n","        ax.set_xlim(-1, len(display_seq))\n","        ax.set_ylim(-0.5, 0.5)\n","        ax.set_yticks([])\n","        ax.set_xticks([])\n","        ax.set_title(f'Step {idx+1}/{num_steps} (α={idx/(num_steps-1):.2f})',\n","                    fontsize=9, loc='left')\n","        ax.spines['top'].set_visible(False)\n","        ax.spines['right'].set_visible(False)\n","        ax.spines['bottom'].set_visible(False)\n","        ax.spines['left'].set_visible(False)\n","\n","    plt.tight_layout()\n","    plt.savefig('latent_interpolation.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"✓ Latent space interpolation visualized\")\n","\n","\n","interpolate_latent_space(model, test_loader, device, num_steps=10)"],"metadata":{"id":"0OnZ8uGLWKvC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 18: Clustering Analysis"],"metadata":{"id":"n0TB4aWFWPUC"}},{"cell_type":"code","source":["def analyze_latent_clustering(latents_dict, n_clusters=10):\n","    \"\"\"\n","    Perform k-means clustering on each latent level.\n","    Measures how well the model self-organizes data.\n","    \"\"\"\n","    from sklearn.cluster import KMeans\n","    from sklearn.metrics import silhouette_score, davies_bouldin_score\n","\n","    results = {}\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    for idx, (level_name, latents) in enumerate(latents_dict.items()):\n","        print(f\"Clustering {level_name}...\")\n","\n","        # Perform k-means\n","        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n","        cluster_labels = kmeans.fit_predict(latents)\n","\n","        # Compute metrics\n","        silhouette = silhouette_score(latents, cluster_labels)\n","        davies_bouldin = davies_bouldin_score(latents, cluster_labels)\n","        inertia = kmeans.inertia_\n","\n","        results[level_name] = {\n","            'silhouette': silhouette,\n","            'davies_bouldin': davies_bouldin,\n","            'inertia': inertia,\n","            'cluster_labels': cluster_labels\n","        }\n","\n","        # Visualize cluster distribution\n","        ax = axes[idx]\n","        unique, counts = np.unique(cluster_labels, return_counts=True)\n","        ax.bar(unique, counts, color='steelblue', alpha=0.8)\n","        ax.set_xlabel('Cluster ID', fontsize=11)\n","        ax.set_ylabel('Number of Samples', fontsize=11)\n","        ax.set_title(f'{level_name.capitalize()} Clustering\\n'\n","                    f'Silhouette: {silhouette:.3f} | DB: {davies_bouldin:.3f}',\n","                    fontsize=11, fontweight='bold')\n","        ax.grid(alpha=0.3, axis='y')\n","\n","    plt.tight_layout()\n","    plt.savefig('clustering_analysis.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    # Print summary\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CLUSTERING ANALYSIS\")\n","    print(\"=\"*60)\n","    for level_name, result in results.items():\n","        print(f\"\\n{level_name.upper()}:\")\n","        print(f\"  Silhouette score:     {result['silhouette']:.4f} (higher is better, range [-1, 1])\")\n","        print(f\"  Davies-Bouldin score: {result['davies_bouldin']:.4f} (lower is better)\")\n","        print(f\"  Inertia:              {result['inertia']:.2f}\")\n","    print(\"=\"*60)\n","\n","    return results\n","\n","\n","clustering_results = analyze_latent_clustering(latents_dict, n_clusters=10)"],"metadata":{"id":"o4QimeB2WVTT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 19: Latent Space Arithmetic"],"metadata":{"id":"nBPA1n2nWkmp"}},{"cell_type":"code","source":["def latent_arithmetic(model, dataloader, device):\n","    \"\"\"\n","    Test if latent space supports meaningful vector arithmetic.\n","    Similar to word2vec's \"king - man + woman = queen\"\n","    \"\"\"\n","    model.eval()\n","\n","    # Get three random samples\n","    batch = next(iter(dataloader)).to(device)\n","    x1, x2, x3 = batch[0:1], batch[1:2], batch[2:3]\n","\n","    with torch.no_grad():\n","        # Encode\n","        latents1, _ = model.encode(x1)\n","        latents2, _ = model.encode(x2)\n","        latents3, _ = model.encode(x3)\n","\n","        # Perform arithmetic: (x1 - x2) + x3\n","        result_latents = tuple(\n","            (z1 - z2) + z3\n","            for z1, z2, z3 in zip(latents1, latents2, latents3)\n","        )\n","\n","        # Decode all\n","        recon1 = model.decode(latents1).cpu().numpy()[0].reshape(4, 1024)\n","        recon2 = model.decode(latents2).cpu().numpy()[0].reshape(4, 1024)\n","        recon3 = model.decode(latents3).cpu().numpy()[0].reshape(4, 1024)\n","        recon_result = model.decode(result_latents).cpu().numpy()[0].reshape(4, 1024)\n","\n","        # Decode to sequences\n","        seq1 = DNAEncoder.decode_one_hot(recon1)\n","        seq2 = DNAEncoder.decode_one_hot(recon2)\n","        seq3 = DNAEncoder.decode_one_hot(recon3)\n","        seq_result = DNAEncoder.decode_one_hot(recon_result)\n","\n","    # Visualize\n","    fig, axes = plt.subplots(4, 1, figsize=(16, 8))\n","    sequences = [seq1, seq2, seq3, seq_result]\n","    labels = ['Sequence A', 'Sequence B', 'Sequence C', 'Result: (A - B) + C']\n","\n","    for ax, seq, label in zip(axes, sequences, labels):\n","        display_seq = seq[:100]\n","\n","        for pos, base in enumerate(display_seq):\n","            color_map = {'A': 'green', 'C': 'blue', 'G': 'orange', 'T': 'red'}\n","            ax.text(pos, 0, base, ha='center', va='center', fontsize=9,\n","                   family='monospace', color=color_map.get(base, 'black'),\n","                   fontweight='bold')\n","\n","        ax.set_xlim(-1, len(display_seq))\n","        ax.set_ylim(-0.5, 0.5)\n","        ax.set_yticks([])\n","        ax.set_xticks([])\n","        ax.set_ylabel(label, fontsize=10, fontweight='bold')\n","        ax.spines['top'].set_visible(False)\n","        ax.spines['right'].set_visible(False)\n","        ax.spines['bottom'].set_visible(False)\n","        ax.spines['left'].set_visible(False)\n","\n","    plt.tight_layout()\n","    plt.savefig('latent_arithmetic.png', dpi=150, bbox_inches='tight')\n","    plt.show()\n","\n","    print(\"✓ Latent arithmetic visualization complete\")\n","\n","\n","latent_arithmetic(model, test_loader, device)"],"metadata":{"id":"65POp0ckWoWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 20: Save Complete Analysis Report"],"metadata":{"id":"zEanWpuxWshc"}},{"cell_type":"code","source":["def generate_analysis_report(history, intrinsic_dims, clustering_results,\n","                            reconstruction_accuracies):\n","    \"\"\"\n","    Generate a comprehensive text report of all analyses.\n","    \"\"\"\n","    report = []\n","    report.append(\"=\"*80)\n","    report.append(\"HIERARCHICAL VAE ANALYSIS REPORT\")\n","    report.append(\"=\"*80)\n","    report.append(\"\")\n","\n","    # Training summary\n","    report.append(\"1. TRAINING SUMMARY\")\n","    report.append(\"-\" * 80)\n","    report.append(f\"   Final training loss:     {history['train_loss'][-1]:.4f}\")\n","    report.append(f\"   Final validation loss:   {history['val_loss'][-1]:.4f}\")\n","    report.append(f\"   Final reconstruction:    {history['train_recon'][-1]:.4f}\")\n","    report.append(f\"   Final KL divergence:     {history['train_kl'][-1]:.4f}\")\n","    report.append(f\"   Total epochs:            {len(history['train_loss'])}\")\n","    report.append(\"\")\n","\n","    # Intrinsic dimensionality\n","    report.append(\"2. INTRINSIC DIMENSIONALITY\")\n","    report.append(\"-\" * 80)\n","    for level, dims in intrinsic_dims.items():\n","        utilization = (dims['intrinsic_dim'] / dims['nominal_dim']) * 100\n","        report.append(f\"   {level}:\")\n","        report.append(f\"     Nominal:    {dims['nominal_dim']}\")\n","        report.append(f\"     Intrinsic:  {dims['intrinsic_dim']}\")\n","        report.append(f\"     Usage:      {utilization:.1f}%\")\n","    report.append(\"\")\n","\n","    # Clustering quality\n","    report.append(\"3. CLUSTERING QUALITY\")\n","    report.append(\"-\" * 80)\n","    for level, results in clustering_results.items():\n","        report.append(f\"   {level}:\")\n","        report.append(f\"     Silhouette:     {results['silhouette']:.4f}\")\n","        report.append(f\"     Davies-Bouldin: {results['davies_bouldin']:.4f}\")\n","    report.append(\"\")\n","\n","    # Reconstruction quality\n","    report.append(\"4. RECONSTRUCTION QUALITY\")\n","    report.append(\"-\" * 80)\n","    report.append(f\"   Mean accuracy:   {np.mean(reconstruction_accuracies):.4f}\")\n","    report.append(f\"   Median accuracy: {np.median(reconstruction_accuracies):.4f}\")\n","    report.append(f\"   Std deviation:   {np.std(reconstruction_accuracies):.4f}\")\n","    report.append(\"\")\n","\n","    report.append(\"=\"*80)\n","    report.append(\"END OF REPORT\")\n","    report.append(\"=\"*80)\n","\n","    # Save to file\n","    report_text = \"\\n\".join(report)\n","    with open('analysis_report.txt', 'w') as f:\n","        f.write(report_text)\n","\n","    print(report_text)\n","    print(\"\\n✓ Report saved to 'analysis_report.txt'\")\n","\n","    return report_text\n","\n","\n","report = generate_analysis_report(\n","    history,\n","    intrinsic_dims,\n","    clustering_results,\n","    reconstruction_accuracies\n",")"],"metadata":{"id":"iFh6X0YoWxf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cell 21: Download All Artifacts"],"metadata":{"id":"sl9UIRP6W08Y"}},{"cell_type":"code","source":["# Download all saved files\n","from google.colab import files\n","\n","print(\"Downloading artifacts...\")\n","\n","artifacts = [\n","    'best_hierarchical_vae.pth',\n","    'training_history.png',\n","    'intrinsic_dimensionality.png',\n","    'latent_space_umap.png',\n","    'latent_space_tsne.png',\n","    'reconstruction_quality.png',\n","    'latent_interpolation.png',\n","    'clustering_analysis.png',\n","    'latent_arithmetic.png',\n","    'analysis_report.txt'\n","]\n","\n","for artifact in artifacts:\n","    try:\n","        files.download(artifact)\n","        print(f\"✓ Downloaded: {artifact}\")\n","    except:\n","        print(f\"✗ Could not download: {artifact}\")\n","\n","print(\"\\n✓ Download complete\")"],"metadata":{"id":"XwLnwr_0W62v"},"execution_count":null,"outputs":[]}]}